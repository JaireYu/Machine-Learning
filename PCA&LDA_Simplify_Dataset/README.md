## 使用PCA简化数据
### 降维技术
#### 目的:
1. 使数据集更易使用
2. 降低算法的计算开销
3. 去除噪声
4. 使结果易懂
#### 主要方法:
1. 主成分分析(PCA)
2. 因子分析(FA)
3. 独立成分分析(ICA)
>下面主要介绍PCA
### PCA
#### PCA的基本思想:
    转换坐标轴, 选择方差最大的方向(主成分)作为第一条坐标轴, 然后选择与之正交方向中方差最大的方向, 以此类推
#### 算法步骤:
  1. 每一个维度去除平均值
  2. 计算协方差矩阵
  3. 计算协方差矩阵的特征向量和特征值
  4. 选择特征值最上面的k个
  5. 将数据转到由对应的k个特征向量构成的空间中完成数据降维
#### 数学解释:
  给定标准正交基下的坐标, 左乘新的基在标准正交基下的坐标矩阵(过渡矩阵的逆)即可得到该向量在新的基向量下的坐标
  $$\left(
  \begin{matrix}  
  p_1 \\
  p_2 \\
    \vdots\\
    p_R
  \end{matrix}
  \right) \cdot \left(
  \begin{matrix}
         a_1& a_2& \cdots & a_M 
  \end{matrix}
  \right) 
  = \left(
  \begin{matrix}
  p_1a_1 & p_1a_2 & \cdots & p_1a_M\\
          p_2a_1 & p_2a_2 & \cdots & p_2a_M\\
          \vdots &\vdots &\ddots &\vdots \\
          p_Ra_1&p_Ra_2&\cdots&p_Ra_M       
  \end{matrix}
  \right)$$ 
  新的基的个数可以小于原来的基的个数($R<N$), 得到的是R维的向量, 所以相当于对数据降维处理了.
  **问题转化为如何选择R个基来最大化地保留原始信息？**
  在直观角度看, 要选择投影后的投影值最为分散的方向, 度量工具为协方差矩阵, 首先将数据均值化, 即对每一个维度的数值都减去均值使之分布在以(0,0)为中心的附近
  在处理之后, 求解方差协方差更加方便:
  **方差**:
   $$Var(a) = \frac{1}{m}\sum_{i=1}^{m}(a_i-\mu)^2 = \frac{1}{m}\sum_{i=1}^{m}a_i^2$$

   **协方差**:
   $$Cov(a,b) = \frac{1}{m}\sum_{i = 1}^{m}a_ib_i$$

   选择第一个方向要选择方差最大的方向, 选择第二个方向应不与第一个方向存在相关性, 否则意味着损失信息.所以要选择协方差为0的方向(即相互正交)
    **协方差矩阵**:
    假设我们有m个n维数据记录(每一列是一个数据), 将其组成n行m列的矩阵, 则第i个维度和第j个维度的协方差可以组成如下矩阵:
    $$
    X=\left(
    \begin{matrix}
          a_{11}&a_{12}&\cdots&a_{1m}\\
          a_{21}&a_{22}&\cdots&a_{2m} \\
          \vdots&\vdots&\ddots&\vdots\\
          a_{n1}&a_{n2}&\cdots&a_{nm} 
    \end{matrix}
    \right)$$
    协方差矩阵可以写作:
    $$
    \frac{1}{m}XX^T = \left(
    \begin{matrix}
        \frac{1}{m}\sum_{i=1}^{m}a_{1i}^2& \frac{1}{m}\sum_{i=1}^{m}a_{1i}a_{2i} & \cdots &\frac{1}{m}\sum_{i=1}^{m}a_{1i}a_{mi}\\
        \frac{1}{m}\sum_{i=1}^{m}a_{2i}a_{1i}& \frac{1}{m}\sum_{i=1}^{m}a_{2i}^2&\cdots&\frac{1}{m}\sum_{i=1}^{m}a_{2i}a_{mi}\\
        \vdots& \vdots & \ddots & \vdots\\
        \frac{1}{m}\sum_{i=1}^{m}a_{mi}a_{1i}&\frac{1}{m}\sum_{i=1}^{m}a_{mi}a_{2i}& \cdots &\frac{1}{m}\sum_{i=1}^{m}a_{mi}^2
    \end{matrix}
    \right)
    $$
    **回顾优化目标**:
    1. 数据降维后每个维度的数据应该相互独立
    2. 数据降维后方差应该尽可能地大
   
**求解变换矩阵$P$**
    
设原始数据矩阵$X$对应的协方差矩阵为$C$，而$P$是一组基按行组成的矩阵，设$Y=PX$，则$Y$为$X$对$P$做基变换后的数据。设$Y$的协方差矩阵为$D$，我们推导一下$D$与$C$的关系:
$$
D = \frac{1}{m}YY^T=\frac{1}{m}PXX^TP^T = PCP^T
$$

根据线性代数知识, 这样的$P$一定存在, 而且使得$D$为对角矩阵(协方差为0), 我们只需要对$C$对角化即可, 协方差矩阵$C$是一个是对称矩阵，在线性代数上，实对称矩阵有一系列非常好的性质：
1）实对称矩阵不同特征值对应的特征向量必然正交。
2）设特征向量$\lambda$重数为$r$，则必然存在$r$个线性无关的特征向量对应于$\lambda$，因此可以将这$r$个特征向量单位正交化。
由上面两条可知，一个n行n列的实对称矩阵一定可以找到n个单位正交特征向量
记得到的n个单位正交向量组为$E=(e_1, e_2, \cdots, e_n )$这样的矩阵$P$就是$E^T$
证明如下:
记特征向量矩阵
$$
\Lambda = \left(
\begin{matrix}
      \lambda_1& & \\
       &\lambda_2 & \\
      & & \ddots \\
      & & &   \lambda_n  
\end{matrix}
\right)
$$
易知:
$$
(e_1, e_2, \cdots, e_n)\Lambda = \left(
\begin{matrix}
      \lambda_1e_1&\lambda_2e_2& \cdots&\lambda_n e_n 
\end{matrix}
\right)=A(e_1, e_2, \cdots, e_n)
$$

即:
$$E\Lambda = AE$$

左乘$E^{-1}$就得到:
$$\Lambda = E^{-1}AE$$

由于$E$中的特征向量单位正交, 所以$E^TE = I$
所以有
$$\Lambda = E^{T}AE$$

综上所述$P = E^T$就是要找的矩阵$P$，我们只需寻找特征向量中最大的k个, 对应的使用$P$中的行构成的矩阵即可.
**注意**: 
通常数据都是横向数据$X^{'}$, 即$X^{'}=X^T$, 求协方差矩阵的方法略有变化, 但是可以由$\mathtt{numpy}$库函数$\mathcal{cov}$来求解, 用$\mathcal{linalg.eig}$得到的$E$也和原来相同, 不同的是如果仍要用横向来组织每一条数据, 就需要求解$Y^{'} = Y^T$
于是有:
$$Y^{'} = Y^T = (PX)^T = X^TP^T = X^{'}E$$

#### PCA与LDA
PCA强调的是方差最大, LDA兼顾了类内的分散程度和类间的分散程度
一般来说，有类别标签优先使用LDA, 没有类别标签优先使用PCA

